# Métricas de Avaliação de Modelos de Aprendizado de Máquina

Este repositório contém implementações de métricas utilizadas para avaliar o desempenho de modelos de aprendizado de máquina. As métricas mais comuns incluem **precisão**, **recall**, **F1-score** e **AUC-ROC**. Essas métricas são fundamentais para medir o sucesso de modelos de classificação e ajudam a entender o equilíbrio entre acertos e erros do modelo.

## Principais Métricas

- **Precisão (Precision)**: Mede a proporção de previsões corretas entre as previsões positivas feitas pelo modelo.
- **Recall**: Mede a capacidade do modelo de identificar todas as instâncias positivas.
- **F1-Score**: Combina precisão e recall em uma única métrica, equilibrando os dois.
- **AUC-ROC**: Mede a capacidade do modelo de distinguir entre as classes positivas e negativas.

Essas métricas são essenciais para avaliar a performance de modelos em problemas de classificação, especialmente quando temos classes desbalanceadas ou quando o custo de falsos positivos e falsos negativos não é igual.

## Resultados
Métricas de Classificação:
Acurácia: 0.71

Precisão: 0.75

Sensibilidade (Recall): 0.75

Especificidade: 0.67

F1-Score: 0.75

Matriz de Confusão:
 [[2 1] 
   [1 3]]



![)](https://github.com/user-attachments/assets/cbcde0c3-970c-433e-8cb9-1930706a0f08)

![9](https://github.com/user-attachments/assets/21c83403-03ca-4c11-855f-c61749a82163)
